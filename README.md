# LattesScraper

Um sistema automatizado para extrair e processar dados de curr√≠culos Lattes utilizando web scraping, intelig√™ncia artificial e processamento de dados estruturados.

## üìã Descri√ß√£o

O LattesScraper √© uma ferramenta desenvolvida para automatizar a coleta e processamento de informa√ß√µes acad√™micas da Plataforma Lattes do CNPq. O sistema realiza scraping dos curr√≠culos, extrai informa√ß√µes relevantes usando IA, e organiza os dados em formatos estruturados (JSON e Excel) para an√°lise posterior.

### Principais Funcionalidades

- **Web Scraping Automatizado**: Coleta dados de curr√≠culos Lattes de forma massiva
- **Contorno de CAPTCHA**: Utiliza API de browser para resolver CAPTCHAs automaticamente
- **Extra√ß√£o via IA**: Processa conte√∫do HTML usando modelos de linguagem para extrair informa√ß√µes estruturadas
- **Processamento Concorrente**: Suporte a m√∫ltiplas requisi√ß√µes simult√¢neas com controle de rate limiting
- **M√∫ltiplos Formatos de Sa√≠da**: Gera dados em JSON e planilhas Excel organizadas
- **Tratamento de Erros**: Sistema robusto de logging e reprocessamento de falhas

## üîß Tecnologias Utilizadas

- **Python 3.13+**: Linguagem principal do projeto
- **Playwright**: Automa√ß√£o de browser para web scraping
- **BeautifulSoup4**: Parsing e manipula√ß√£o de HTML
- **OpenAI/Instructor**: Extra√ß√£o estruturada via modelos de linguagem
- **Polars**: Processamento eficiente de dados
- **Pydantic**: Valida√ß√£o e serializa√ß√£o de dados
- **XlsxWriter**: Gera√ß√£o de planilhas Excel
- **Tqdm**: Barras de progresso para opera√ß√µes ass√≠ncronas

## üìÅ Estrutura do Projeto

```
LattesScraper/
‚îú‚îÄ‚îÄ .venv/                    # Ambiente virtual Python
‚îú‚îÄ‚îÄ resumes/                  # Arquivos HTML dos curr√≠culos coletados
‚îú‚îÄ‚îÄ jsons/                    # Dados extra√≠dos em formato JSON bruto
‚îú‚îÄ‚îÄ enriched_jsons/           # Dados processados e enriquecidos via IA
‚îú‚îÄ‚îÄ excel/                    # Planilhas Excel organizadas por pesquisador
‚îú‚îÄ‚îÄ failed_requests/          # Logs de requisi√ß√µes com falha
‚îú‚îÄ‚îÄ scrape.ipynb             # Notebook principal para web scraping
‚îú‚îÄ‚îÄ parse.ipynb              # Notebook para processamento e extra√ß√£o via IA
‚îú‚îÄ‚îÄ resumes.txt              # Lista de URLs dos curr√≠culos Lattes
‚îú‚îÄ‚îÄ failed_extractions.txt   # URLs que falharam durante o processo
‚îú‚îÄ‚îÄ pyproject.toml           # Configura√ß√£o do projeto e depend√™ncias
‚îî‚îÄ‚îÄ README.md                # Este arquivo
```

## üöÄ Instala√ß√£o e Configura√ß√£o

### Pr√©-requisitos

- Python 3.13 ou superior
- UV (gerenciador de pacotes Python) - recomendado
- Conta no OpenRouter ou provedor de IA compat√≠vel
- API de Browser (BrightData recomendado) para contorno de CAPTCHA

### Instala√ß√£o

1. **Clone o reposit√≥rio**:

   ```bash
   git clone <url-do-repositorio>
   cd LattesScraper
   ```

2. **Configure o ambiente virtual e instale depend√™ncias**:

   ```bash
   uv venv
   source .venv/bin/activate  # Linux/Mac
   # ou .venv\Scripts\activate  # Windows
   uv sync
   ```

3. **Configure as vari√°veis de ambiente**:

   ```bash
   # Crie um arquivo .env com:
   CDP_URL=sua_url_de_browser_api_aqui
   OPENROUTER_API_KEY=sua_chave_openrouter_aqui
   ```

### Configura√ß√£o de APIs Externas

#### BrightData Browser API

- Registre-se em [BrightData](https://brightdata.com)
- Configure um Browser API endpoint
- Use a URL CDP fornecida na vari√°vel `CDP_URL`

#### OpenRouter API

- Registre-se em [OpenRouter](https://openrouter.ai)
- Obtenha sua chave de API
- Configure na vari√°vel `OPENROUTER_API_KEY`

## üìñ Como Usar

### 1. Prepara√ß√£o dos Dados

Crie o arquivo `resumes.txt` com as URLs dos curr√≠culos Lattes (uma por linha):

```
http://lattes.cnpq.br/8894044025019842
http://lattes.cnpq.br/6290785760473426
http://lattes.cnpq.br/3201859120142095
```

### 2. Execu√ß√£o do Web Scraping

Execute o notebook `scrape.ipynb` para coletar os dados:

```python
# O notebook ir√°:
# 1. Ler as URLs do arquivo resumes.txt
# 2. Conectar com a API de browser
# 3. Navegar e resolver CAPTCHAs automaticamente
# 4. Salvar os HTMLs na pasta resumes/
# 5. Registrar falhas no arquivo failed_extractions.txt
```

### 3. Processamento e Extra√ß√£o

Execute o notebook `parse.ipynb` para extrair informa√ß√µes estruturadas:

```python
# O notebook ir√°:
# 1. Processar arquivos HTML da pasta resumes/
# 2. Usar IA para extrair publica√ß√µes e bancas acad√™micas
# 3. Salvar dados brutos em jsons/
# 4. Enriquecer dados e salvar em enriched_jsons/
# 5. Gerar planilhas Excel individuais em excel/
```

## üìä Estrutura dos Dados Extra√≠dos

### Publica√ß√µes (`Publicacao`)

```python
{
    "ano": int,              # Ano da publica√ß√£o
    "titulo": str,           # T√≠tulo da publica√ß√£o
    "autores": str,          # Lista de autores (separados por v√≠rgula)
    "periodico": str | None, # Nome do peri√≥dico
    "editora": str | None,   # Editora ou ve√≠culo de publica√ß√£o
    "cidade": str | None,    # Cidade de publica√ß√£o
    "extra": str | None      # Informa√ß√µes adicionais
}
```

### Bancas Acad√™micas (`Banca`)

```python
{
    "ano": int,              # Ano da banca
    "titulo": str,           # T√≠tulo do trabalho avaliado
    "membros": str,          # Lista de membros (separados por v√≠rgula)
    "candidato": str | None, # Nome do candidato
    "instituicao": str | None # Institui√ß√£o de origem
}
```

## üîß Configura√ß√µes Avan√ßadas

### Controle de Concorr√™ncia

No arquivo `scrape.ipynb`, ajuste o sem√°foro para controlar requisi√ß√µes simult√¢neas:

```python
semaphore = Semaphore(5)  # M√°ximo 5 requisi√ß√µes simult√¢neas
```

### Modelo de IA

No arquivo `parse.ipynb`, configure o modelo desejado:

```python
default_model = "mistralai/mistral-medium-3"  # Ou outro modelo compat√≠vel
```

### Timeout e Retry

Configure timeouts e tentativas no c√≥digo:

```python
await page.goto(url, timeout=2 * 60_000)  # 2 minutos de timeout
```

## üêõ Solu√ß√£o de Problemas

### Erros Comuns

1. **Erro de CAPTCHA**: Verifique se a API de browser est√° configurada corretamente
2. **Rate Limiting**: Reduza o n√∫mero de requisi√ß√µes simult√¢neas no sem√°foro
3. **Timeout**: Aumente os valores de timeout para conex√µes lentas
4. **Falhas de IA**: Verifique se a chave da API OpenRouter est√° v√°lida

### Logs e Monitoramento

- URLs com falha s√£o registradas em `failed_extractions.txt`
- Progresso √© exibido via barras do tqdm
- Logs detalhados s√£o impressos durante a execu√ß√£o

## üìà Reprocessamento de Falhas

Para reprocessar URLs que falharam:

1. Verifique o arquivo `failed_extractions.txt`
2. Copie as URLs com falha para `resumes.txt`
3. Execute novamente os notebooks
