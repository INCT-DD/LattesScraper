{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import instructor\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code processes the HTML files scraped from the Lattes platform. Because the HTML structure of the Lattes platform is not very consistent, the code has to be adapted to the specific structure of the HTML file. To make things easier, we'll use AI to help extract the information. Here we use OpenRouter, but you can also use a sufficient LLM from another provider or even a local LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openrouter_api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if not openrouter_api_key:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY is not set\")\n",
    "default_model = \"google/gemini-2.0-flash-lite-001\"\n",
    "openai_client = AsyncOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\", api_key=openrouter_api_key\n",
    ")\n",
    "instructor_client = instructor.from_openai(openai_client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let setup the Pydantic model for the article. This will guide the LLM to extract the information in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article(BaseModel, extra=\"allow\"):\n",
    "    authors: str = Field(\n",
    "        description=\"A lista de autores do artigo, separados por vírgulas\"\n",
    "    )\n",
    "    title: str = Field(description=\"O título do artigo\")\n",
    "    journal: str = Field(description=\"O periódico do artigo\")\n",
    "    city: str = Field(description=\"A cidade do artigo\")\n",
    "    year: int = Field(description=\"O ano do artigo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Você é um assistente prestativo que extrai informações de um texto fornecido. Você receberá o texto de uma citação e precisará extrair as informações no formato especificado pelo esquema.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a function to extract the clean text from the HTML element. This function will be used to extract the text from the HTML element and return it as a string, ignoring images and other unwanted content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_text(element):\n",
    "    \"\"\"\n",
    "    Extract clean text content from an HTML element, ignoring images and other unwanted content.\n",
    "    \"\"\"\n",
    "    if not element:\n",
    "        return \"\"\n",
    "\n",
    "    temp_element = element.__copy__()\n",
    "\n",
    "    unwanted_tags = [\"img\", \"sup\"]\n",
    "    for tag in unwanted_tags:\n",
    "        for unwanted in temp_element.find_all(tag):\n",
    "            unwanted.decompose()\n",
    "\n",
    "    text = temp_element.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML structure of the Lattes platform is not very consistent, so this isn't a generic function. For each section we need to extract the citations from, you'll need to provide the correct CSS selector. This approach is not scalable and very prone to break, but it works for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_publications(container):\n",
    "    \"\"\"\n",
    "    Extract all spans with class 'transform' between consecutive 'cita-artigos' divs\n",
    "    and organize them by section.\n",
    "    \"\"\"\n",
    "    publications = []\n",
    "\n",
    "    # Find all divs with class \"cita-artigos\"\n",
    "    cita_artigos_divs = container.find_all(\"div\", class_=\"cita-artigos\")\n",
    "\n",
    "    for i, current_div in enumerate(cita_artigos_divs):\n",
    "        section_name = extract_clean_text(current_div)\n",
    "        citations = []\n",
    "\n",
    "        # Start from the current div and look for the next sibling elements\n",
    "        current_element = current_div.next_sibling\n",
    "\n",
    "        # Continue until we find the next \"cita-artigos\" div or reach the end\n",
    "        while current_element:\n",
    "            # If we encounter another \"cita-artigos\" div, stop\n",
    "            if (\n",
    "                hasattr(current_element, \"get\")\n",
    "                and current_element.get(\"class\")\n",
    "                and \"cita-artigos\" in current_element.get(\"class\", [])\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            # If current element has descendants, search for transform spans\n",
    "            if hasattr(current_element, \"find_all\"):\n",
    "                transform_spans = current_element.find_all(\"span\", class_=\"transform\")\n",
    "                for span in transform_spans:\n",
    "                    citation_text = extract_clean_text(span)\n",
    "                    if citation_text:  # Only add non-empty citations\n",
    "                        citations.append(citation_text)\n",
    "\n",
    "            # Move to the next sibling\n",
    "            current_element = current_element.next_sibling\n",
    "\n",
    "        # Add to publications list\n",
    "        publications.append({\"name\": section_name, \"citations\": citations})\n",
    "\n",
    "    return publications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(soup):\n",
    "    info_container = soup.find(\"div\", class_=\"infpessoa\")\n",
    "    name = info_container.find(\"h2\", class_=\"nome\").text.strip()\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_resumes():\n",
    "    \"\"\"Process all HTML files in the resumes folder and extract data to JSON files\"\"\"\n",
    "\n",
    "    # Get all HTML files in the resumes folder\n",
    "    resumes_path = Path(\"./resumes\")\n",
    "    html_files = list(resumes_path.glob(\"*.html\"))\n",
    "\n",
    "    if not html_files:\n",
    "        print(\"No HTML files found in the resumes folder\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(html_files)} HTML files to process\")\n",
    "\n",
    "    # Ensure jsons directory exists\n",
    "    jsons_path = Path(\"./jsons\")\n",
    "    jsons_path.mkdir(exist_ok=True)\n",
    "\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    failed_files = []\n",
    "\n",
    "    for html_file in html_files:\n",
    "        try:\n",
    "            print(f\"Processing: {html_file.name}\")\n",
    "\n",
    "            # Read the HTML file\n",
    "            with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                html_content = file.read()\n",
    "\n",
    "            soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "            # Extract containers using the same selectors\n",
    "            publications_container = soup.select_one(\n",
    "                'div.title-wrapper > a[name=\"ProducoesCientificas\"]:first-child'\n",
    "            )\n",
    "\n",
    "            if publications_container is not None:\n",
    "                publications_container = publications_container.parent\n",
    "\n",
    "            committees_container = soup.select_one(\n",
    "                'div.title-wrapper > a[name=\"Bancas\"]:first-child'\n",
    "            )\n",
    "\n",
    "            if committees_container is not None:\n",
    "                committees_container = committees_container.parent\n",
    "\n",
    "            # Extract data\n",
    "            data = {\n",
    "                \"name\": extract_name(soup),\n",
    "                \"publications\": extract_publications(publications_container)\n",
    "                if publications_container\n",
    "                else [],\n",
    "                \"committees\": extract_publications(committees_container)\n",
    "                if committees_container\n",
    "                else [],\n",
    "            }\n",
    "\n",
    "            # Generate output filename using the same base name\n",
    "            json_filename = f\"{html_file.stem}.json\"\n",
    "            json_output_path = jsons_path / json_filename\n",
    "\n",
    "            # Save to JSON\n",
    "            with open(json_output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "            successful += 1\n",
    "            print(f\"  ✓ Saved to {json_output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            failed_files.append((html_file.name, str(e)))\n",
    "            print(f\"  ✗ Failed to process {html_file.name}: {e}\")\n",
    "\n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "\n",
    "    if failed_files:\n",
    "        print(\"\\nFailed files:\")\n",
    "        for filename, error in failed_files:\n",
    "            print(f\"  {filename}: {error}\")\n",
    "\n",
    "\n",
    "# Run the processing\n",
    "process_all_resumes()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
