{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import stamina\n",
    "import instructor\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import AsyncOpenAI\n",
    "from xlsxwriter import Workbook\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code processes the HTML files scraped from the Lattes platform. Because the HTML structure of the Lattes platform is not very consistent, the code has to be adapted to the specific structure of the HTML file. To make things easier, we'll use AI to help extract the information. Here we use OpenRouter, but you can also use a sufficient LLM from another provider or even a local LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "openrouter_api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if not openrouter_api_key:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY is not set\")\n",
    "default_model = \"mistralai/mistral-medium-3\"\n",
    "openai_client = AsyncOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\", api_key=openrouter_api_key\n",
    ")\n",
    "instructor_client = instructor.from_openai(openai_client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let setup the Pydantic models for the publications and committees. This will guide the LLM to extract the information in the correct format. As the data extracted is in Portuguese, we'll use Portuguese to define the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Publicacao(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model representing a scientific publication extracted from Lattes CV.\n",
    "\n",
    "    This model defines the structure for publications including journal articles,\n",
    "    book chapters, and other academic works found in Brazilian Lattes CVs.\n",
    "\n",
    "    Attributes:\n",
    "        ano: The year the publication was published\n",
    "        titulo: The title of the publication\n",
    "        autores: Comma-separated list of authors\n",
    "        periodico: The journal/periodical name (optional)\n",
    "        editora: The publisher or media outlet (optional)\n",
    "        cidade: The city of publication (optional)\n",
    "        extra: Additional information about the publication format or type (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    ano: int = Field(description=\"O ano da publicação.\")\n",
    "    titulo: str = Field(description=\"O título da publicação.\")\n",
    "    autores: str = Field(\n",
    "        description=\"A lista de autores da publicação, separados por vírgulas.\"\n",
    "    )\n",
    "    periodico: str | None = Field(\n",
    "        description=\"O periódico da publicação. Não deve incluir a cidade. Ex.: para 'Civitas (Porto Alegre)' o periódico é 'Civitas'\"\n",
    "    )\n",
    "    editora: str | None = Field(\n",
    "        description=\"A editora da publicação. Este campo pode ser None caso a publicação não tenha uma editora. Para alguns tipos de publicação isso pode ser uma emissora de rádio ou televisão ou outro tipo de mídia.\"\n",
    "    )\n",
    "    cidade: str | None = Field(\n",
    "        description=\"A cidade ou local da publicação e/ou editora. Ex.: para 'Civitas (Porto Alegre)' o local é 'Porto Alegre'.\"\n",
    "    )\n",
    "    extra: str | None = Field(\n",
    "        description=\"Algumas publicações podem incluir informações adicionais que não estão presentes nos outros campos. Por exemplo, descrições mais detalhadas sobre o tipo de publicação, o formato, etc. Ex.: 'Material Didático em PPT sobre Comunicação, Arte e Reprodutibilidade Técnica'.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Banca(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model representing an academic committee/examination board (banca).\n",
    "\n",
    "    This model defines the structure for thesis defense committees, qualification\n",
    "    exams, and other academic evaluation boards found in Brazilian Lattes CVs.\n",
    "\n",
    "    Attributes:\n",
    "        ano: The year the committee evaluation took place\n",
    "        titulo: The title of the work or project being evaluated\n",
    "        membros: Comma-separated list of committee members\n",
    "        candidato: The name of the candidate being evaluated (optional)\n",
    "        instituicao: The institution where the evaluation took place (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    ano: int = Field(description=\"O ano em que a banca foi realizada.\")\n",
    "    titulo: str = Field(\n",
    "        description=\"O título do trabalho avaliado pela banca ou o nome do projeto.\"\n",
    "    )\n",
    "    membros: str = Field(\n",
    "        description=\"A lista de membros da banca, separados por vírgulas.\"\n",
    "    )\n",
    "    candidato: str | None = Field(description=\"O nome do candidato que foi avaliado.\")\n",
    "    instituicao: str | None = Field(\n",
    "        description=\"A instituição onde a banca foi realizada, onde o projeto foi desenvolvido ou onde o trabalho foi realizado.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Você é um assistente prestativo que extrai informações de um texto fornecido. Você receberá o texto de uma referência bibliográfica e precisará extrair as informações no formato especificado pelo esquema.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a function to extract the clean text from the HTML element. This function will be used to extract the text from the HTML element and return it as a string, ignoring images and other unwanted content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_text(element):\n",
    "    \"\"\"\n",
    "    Extract clean text content from an HTML element, removing unwanted tags and formatting.\n",
    "\n",
    "    This function processes BeautifulSoup HTML elements to extract readable text,\n",
    "    removing images, superscript tags, and excess whitespace while preserving\n",
    "    the essential textual content.\n",
    "\n",
    "    Args:\n",
    "        element: A BeautifulSoup HTML element to extract text from\n",
    "\n",
    "    Returns:\n",
    "        str: Clean text content with normalized whitespace, or empty string if element is None\n",
    "\n",
    "    Note:\n",
    "        Currently removes 'img' and 'sup' tags. Modify unwanted_tags list to remove\n",
    "        additional HTML elements as needed.\n",
    "    \"\"\"\n",
    "    if not element:\n",
    "        return \"\"\n",
    "\n",
    "    temp_element = element.__copy__()\n",
    "\n",
    "    unwanted_tags = [\"img\", \"sup\"]\n",
    "    for tag in unwanted_tags:\n",
    "        for unwanted in temp_element.find_all(tag):\n",
    "            unwanted.decompose()\n",
    "\n",
    "    text = temp_element.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML structure of the Lattes platform is not very consistent, so this isn't a generic function. For each section we need to extract the citations from, you'll need to provide the correct CSS selector. This approach is not scalable and very prone to break, but it works for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_publications(container):\n",
    "    \"\"\"\n",
    "    Extract publication citations organized by sections from a Lattes CV HTML container.\n",
    "\n",
    "    This function navigates the HTML structure of Lattes CVs to find citation sections\n",
    "    marked by 'cita-artigos' divs and extracts all citation spans between consecutive\n",
    "    section headers.\n",
    "\n",
    "    Args:\n",
    "        container: BeautifulSoup HTML element containing the publications section\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: List of dictionaries, each containing:\n",
    "            - 'name' (str): Section name from the 'cita-artigos' div\n",
    "            - 'citations' (list[str]): List of clean citation texts from 'transform' spans\n",
    "\n",
    "    Note:\n",
    "        This function is specifically designed for the Lattes platform HTML structure\n",
    "        and may not work with other CV formats. The HTML structure is inconsistent,\n",
    "        so this approach may need updates if the platform changes.\n",
    "    \"\"\"\n",
    "    publications = []\n",
    "\n",
    "    # Find all divs with class \"cita-artigos\"\n",
    "    cita_artigos_divs = container.find_all(\"div\", class_=\"cita-artigos\")\n",
    "\n",
    "    for i, current_div in enumerate(cita_artigos_divs):\n",
    "        section_name = extract_clean_text(current_div)\n",
    "        citations = []\n",
    "\n",
    "        # Start from the current div and look for the next sibling elements\n",
    "        current_element = current_div.next_sibling\n",
    "\n",
    "        # Continue until we find the next \"cita-artigos\" div or reach the end\n",
    "        while current_element:\n",
    "            # If we encounter another \"cita-artigos\" div, stop\n",
    "            if (\n",
    "                hasattr(current_element, \"get\")\n",
    "                and current_element.get(\"class\")\n",
    "                and \"cita-artigos\" in current_element.get(\"class\", [])\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            # If current element has descendants, search for transform spans\n",
    "            if hasattr(current_element, \"find_all\"):\n",
    "                transform_spans = current_element.find_all(\"span\", class_=\"transform\")\n",
    "                for span in transform_spans:\n",
    "                    citation_text = extract_clean_text(span)\n",
    "                    if citation_text:  # Only add non-empty citations\n",
    "                        citations.append(citation_text)\n",
    "\n",
    "            # Move to the next sibling\n",
    "            current_element = current_element.next_sibling\n",
    "\n",
    "        # Add to publications list\n",
    "        publications.append({\"name\": section_name, \"citations\": citations})\n",
    "\n",
    "    return publications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(soup) -> str:\n",
    "    \"\"\"\n",
    "    Extract the person's name from a Lattes CV HTML document.\n",
    "\n",
    "    This function locates the personal information section in a Lattes CV\n",
    "    and extracts the person's name from the designated header element.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object representing the parsed HTML document\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted name with whitespace stripped\n",
    "\n",
    "    Raises:\n",
    "        AttributeError: If the expected HTML structure (infpessoa div or nome h2) is not found\n",
    "\n",
    "    Note:\n",
    "        This function expects the standard Lattes CV HTML structure with\n",
    "        a div.infpessoa containing an h2.nome element.\n",
    "    \"\"\"\n",
    "    info_container = soup.find(\"div\", class_=\"infpessoa\")\n",
    "    name = info_container.find(\"h2\", class_=\"nome\").text.strip()\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_resumes():\n",
    "    \"\"\"\n",
    "    Process all HTML files in the resumes folder and extract structured data to JSON files.\n",
    "\n",
    "    This function performs batch processing of Lattes CV HTML files, extracting\n",
    "    publications and committee information from each file and saving the results\n",
    "    as JSON files in the jsons directory.\n",
    "\n",
    "    The function:\n",
    "    1. Scans the ./resumes directory for HTML files\n",
    "    2. For each file, extracts name, publications, and committees data\n",
    "    3. Saves structured data as JSON files with the same base filename\n",
    "    4. Reports processing statistics and any failures\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Side Effects:\n",
    "        - Creates ./jsons directory if it doesn't exist\n",
    "        - Writes JSON files to ./jsons directory\n",
    "        - Prints processing status and statistics to console\n",
    "\n",
    "    Note:\n",
    "        Failed extractions are logged to console with error details.\n",
    "        The function continues processing even if individual files fail.\n",
    "    \"\"\"\n",
    "    resumes_path = Path(\"./resumes\")\n",
    "    html_files = list(resumes_path.glob(\"*.html\"))\n",
    "\n",
    "    if not html_files:\n",
    "        print(\"No HTML files found in the resumes folder\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(html_files)} HTML files to process\")\n",
    "\n",
    "    jsons_path = Path(\"./jsons\")\n",
    "    jsons_path.mkdir(exist_ok=True)\n",
    "\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    failed_files = []\n",
    "\n",
    "    for html_file in html_files:\n",
    "        try:\n",
    "            print(f\"Processing: {html_file.name}\")\n",
    "\n",
    "            with open(html_file, \"r\") as file:\n",
    "                html_content = file.read()\n",
    "\n",
    "            soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "            publications_container = soup.select_one(\n",
    "                'div.title-wrapper > a[name=\"ProducoesCientificas\"]:first-child'\n",
    "            )\n",
    "\n",
    "            if publications_container is not None:\n",
    "                publications_container = publications_container.parent\n",
    "\n",
    "            committees_container = soup.select_one(\n",
    "                'div.title-wrapper > a[name=\"Bancas\"]:first-child'\n",
    "            )\n",
    "\n",
    "            if committees_container is not None:\n",
    "                committees_container = committees_container.parent\n",
    "\n",
    "            data = {\n",
    "                \"name\": extract_name(soup),\n",
    "                \"publications\": extract_publications(publications_container)\n",
    "                if publications_container\n",
    "                else [],\n",
    "                \"committees\": extract_publications(committees_container)\n",
    "                if committees_container\n",
    "                else [],\n",
    "            }\n",
    "\n",
    "            json_filename = f\"{html_file.stem}.json\"\n",
    "            json_output_path = jsons_path / json_filename\n",
    "\n",
    "            with open(json_output_path, \"w\") as file:\n",
    "                json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "            successful += 1\n",
    "            print(f\"  ✓ Saved to {json_output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            failed_files.append((html_file.name, str(e)))\n",
    "            print(f\"  ✗ Failed to process {html_file.name}: {e}\")\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "\n",
    "    if failed_files:\n",
    "        print(\"\\nFailed files:\")\n",
    "        for filename, error in failed_files:\n",
    "            print(f\"  {filename}: {error}\")\n",
    "\n",
    "\n",
    "process_all_resumes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "\n",
    "@stamina.retry(on=Exception, attempts=5)\n",
    "async def process_citation(citation):\n",
    "    async with semaphore:\n",
    "        response = await instructor_client.chat.completions.create(\n",
    "            model=default_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": citation},\n",
    "            ],\n",
    "            response_model=Publicacao,\n",
    "        )\n",
    "        return response.model_dump()\n",
    "\n",
    "\n",
    "async def process_committees(citation):\n",
    "    async with semaphore:\n",
    "        response = await instructor_client.chat.completions.create(\n",
    "            model=default_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": citation},\n",
    "            ],\n",
    "            response_model=Banca,\n",
    "        )\n",
    "        return response.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_json(json_data: dict, filename: str = \"\") -> tuple[dict, list[dict]]:\n",
    "    \"\"\"\n",
    "    Processes the citations in the JSON data using tqdm_asyncio.gather for concurrent processing with progress tracking.\n",
    "\n",
    "    Args:\n",
    "        json_data (dict): The parsed JSON data.\n",
    "        filename (str): The source filename for tracking failed requests.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict, list[dict]]: A tuple containing the enriched JSON data and a list of failed requests.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    citation_mappings = []\n",
    "    failed_requests = []\n",
    "\n",
    "    if \"publications\" in json_data:\n",
    "        for pub_idx, publication_category in enumerate(json_data[\"publications\"]):\n",
    "            if \"citations\" in publication_category:\n",
    "                for cite_idx, citation in enumerate(publication_category[\"citations\"]):\n",
    "                    task = process_citation(citation)\n",
    "                    tasks.append(task)\n",
    "                    citation_mappings.append(\n",
    "                        (\n",
    "                            \"publications\",\n",
    "                            pub_idx,\n",
    "                            cite_idx,\n",
    "                            citation,\n",
    "                            publication_category.get(\"name\", \"Unknown Section\"),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    if \"committees\" in json_data:\n",
    "        for comm_idx, committee_category in enumerate(json_data[\"committees\"]):\n",
    "            if \"citations\" in committee_category:\n",
    "                for cite_idx, citation in enumerate(committee_category[\"citations\"]):\n",
    "                    task = process_committees(citation)\n",
    "                    tasks.append(task)\n",
    "                    citation_mappings.append(\n",
    "                        (\n",
    "                            \"committees\",\n",
    "                            comm_idx,\n",
    "                            cite_idx,\n",
    "                            citation,\n",
    "                            committee_category.get(\"name\", \"Unknown Section\"),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    if tasks:\n",
    "        # Use return_exceptions=True to get both results and exceptions\n",
    "        results = await tqdm_asyncio.gather(*tasks, desc=\"Processing citations\")\n",
    "\n",
    "        for result, (\n",
    "            category_type,\n",
    "            category_idx,\n",
    "            citation_idx,\n",
    "            citation_text,\n",
    "            section_name,\n",
    "        ) in zip(results, citation_mappings):\n",
    "            if isinstance(result, Exception):\n",
    "                # Track failed request for retry\n",
    "                failed_request = {\n",
    "                    \"filename\": filename,\n",
    "                    \"citation_text\": citation_text,\n",
    "                    \"category_type\": category_type,\n",
    "                    \"category_idx\": category_idx,\n",
    "                    \"citation_idx\": citation_idx,\n",
    "                    \"section_name\": section_name,\n",
    "                    \"error\": str(result),\n",
    "                    \"error_type\": type(result).__name__,\n",
    "                    \"timestamp\": asyncio.get_event_loop().time(),\n",
    "                }\n",
    "                failed_requests.append(failed_request)\n",
    "                print(f\"Error processing citation in {section_name}: {result}\")\n",
    "                continue\n",
    "\n",
    "            if category_type == \"publications\":\n",
    "                if not isinstance(\n",
    "                    json_data[\"publications\"][category_idx][\"citations\"], list\n",
    "                ):\n",
    "                    json_data[\"publications\"][category_idx][\"citations\"] = [None] * len(\n",
    "                        json_data[\"publications\"][category_idx][\"citations\"]\n",
    "                    )\n",
    "                json_data[\"publications\"][category_idx][\"citations\"][citation_idx] = (\n",
    "                    result\n",
    "                )\n",
    "            else:\n",
    "                if not isinstance(\n",
    "                    json_data[\"committees\"][category_idx][\"citations\"], list\n",
    "                ):\n",
    "                    json_data[\"committees\"][category_idx][\"citations\"] = [None] * len(\n",
    "                        json_data[\"committees\"][category_idx][\"citations\"]\n",
    "                    )\n",
    "                json_data[\"committees\"][category_idx][\"citations\"][citation_idx] = (\n",
    "                    result\n",
    "                )\n",
    "\n",
    "    return json_data, failed_requests\n",
    "\n",
    "\n",
    "async def process_all_jsons():\n",
    "    \"\"\"\n",
    "    Process all JSON files in the jsons folder and save enriched versions to enriched_jsons folder.\n",
    "    Also tracks failed requests for later retry.\n",
    "    \"\"\"\n",
    "    jsons_path = Path(\"./jsons\")\n",
    "    enriched_path = Path(\"./enriched_jsons\")\n",
    "    failed_requests_path = Path(\"./failed_requests\")\n",
    "\n",
    "    enriched_path.mkdir(exist_ok=True)\n",
    "    failed_requests_path.mkdir(exist_ok=True)\n",
    "\n",
    "    json_files = list(jsons_path.glob(\"*.json\"))\n",
    "\n",
    "    if not json_files:\n",
    "        print(\"No JSON files found in the jsons folder\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(json_files)} JSON files to process\")\n",
    "\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    failed_files = []\n",
    "    all_failed_requests = []\n",
    "\n",
    "    for json_file in tqdm_asyncio(json_files, desc=\"Processing JSON files\"):\n",
    "        try:\n",
    "            print(f\"\\nProcessing: {json_file.name}\")\n",
    "\n",
    "            with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            enriched_data, failed_requests = await process_json(data, json_file.name)\n",
    "\n",
    "            # Add failed requests to the global list\n",
    "            all_failed_requests.extend(failed_requests)\n",
    "\n",
    "            enriched_filename = json_file.name\n",
    "            enriched_output_path = enriched_path / enriched_filename\n",
    "\n",
    "            with open(enriched_output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(\n",
    "                    enriched_data, file, indent=4, ensure_ascii=False, default=str\n",
    "                )\n",
    "\n",
    "            # Save individual file's failed requests\n",
    "            if failed_requests:\n",
    "                failed_requests_file = (\n",
    "                    failed_requests_path / f\"{json_file.stem}_failed.json\"\n",
    "                )\n",
    "                with open(failed_requests_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                    json.dump(failed_requests, file, indent=4, ensure_ascii=False)\n",
    "                print(\n",
    "                    f\"  ⚠ {len(failed_requests)} failed citations saved to {failed_requests_file}\"\n",
    "                )\n",
    "\n",
    "            successful += 1\n",
    "            print(f\"  ✓ Saved enriched data to {enriched_output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            failed_files.append((json_file.name, str(e)))\n",
    "            print(f\"  ✗ Failed to process {json_file.name}: {e}\")\n",
    "\n",
    "    # Save all failed requests in a single file for batch retry\n",
    "    if all_failed_requests:\n",
    "        all_failed_requests_file = Path(\"./failed_requests_all.json\")\n",
    "        with open(all_failed_requests_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(all_failed_requests, file, indent=4, ensure_ascii=False)\n",
    "        print(\n",
    "            f\"\\n📝 Total of {len(all_failed_requests)} failed citation requests saved to {all_failed_requests_file}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Total failed citations: {len(all_failed_requests)}\")\n",
    "\n",
    "    if failed_files:\n",
    "        print(\"\\nFailed files:\")\n",
    "        for filename, error in failed_files:\n",
    "            print(f\"  {filename}: {error}\")\n",
    "\n",
    "        with open(\"failed_extractions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for filename, error in failed_files:\n",
    "                f.write(f\"{filename}: {error}\\n\")\n",
    "\n",
    "\n",
    "await process_all_jsons()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retry_failed_requests(\n",
    "    failed_requests_file: str = \"./failed_requests_all.json\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Retry processing failed citation requests from a saved file.\n",
    "\n",
    "    Args:\n",
    "        failed_requests_file (str): Path to the JSON file containing failed requests.\n",
    "    \"\"\"\n",
    "    failed_requests_path = Path(failed_requests_file)\n",
    "\n",
    "    if not failed_requests_path.exists():\n",
    "        print(f\"Failed requests file not found: {failed_requests_file}\")\n",
    "        return\n",
    "\n",
    "    with open(failed_requests_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        failed_requests = json.load(file)\n",
    "\n",
    "    if not failed_requests:\n",
    "        print(\"No failed requests to retry\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(failed_requests)} failed requests to retry\")\n",
    "\n",
    "    successful_retries = []\n",
    "    still_failed = []\n",
    "\n",
    "    for request in tqdm_asyncio(failed_requests, desc=\"Retrying failed requests\"):\n",
    "        try:\n",
    "            if request[\"category_type\"] == \"publications\":\n",
    "                result = await process_citation(request[\"citation_text\"])\n",
    "            else:\n",
    "                result = await process_committees(request[\"citation_text\"])\n",
    "\n",
    "            # Add the successful result to the retry list\n",
    "            successful_retries.append(\n",
    "                {\n",
    "                    **request,\n",
    "                    \"retry_result\": result,\n",
    "                    \"retry_timestamp\": asyncio.get_event_loop().time(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            # Still failed, add to still_failed list\n",
    "            still_failed.append(\n",
    "                {\n",
    "                    **request,\n",
    "                    \"retry_error\": str(e),\n",
    "                    \"retry_error_type\": type(e).__name__,\n",
    "                    \"retry_timestamp\": asyncio.get_event_loop().time(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Save results\n",
    "    if successful_retries:\n",
    "        successful_retries_file = Path(\"./successful_retries.json\")\n",
    "        with open(successful_retries_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(successful_retries, file, indent=4, ensure_ascii=False)\n",
    "        print(\n",
    "            f\"✓ {len(successful_retries)} successful retries saved to {successful_retries_file}\"\n",
    "        )\n",
    "\n",
    "    if still_failed:\n",
    "        still_failed_file = Path(\"./still_failed_requests.json\")\n",
    "        with open(still_failed_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(still_failed, file, indent=4, ensure_ascii=False)\n",
    "        print(\n",
    "            f\"⚠ {len(still_failed)} requests still failed, saved to {still_failed_file}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nRetry complete!\")\n",
    "    print(f\"Successful retries: {len(successful_retries)}\")\n",
    "    print(f\"Still failed: {len(still_failed)}\")\n",
    "\n",
    "\n",
    "async def apply_successful_retries(retries_file: str = \"./successful_retries.json\"):\n",
    "    \"\"\"\n",
    "    Apply successful retry results back to the enriched JSON files.\n",
    "\n",
    "    Args:\n",
    "        retries_file (str): Path to the JSON file containing successful retries.\n",
    "    \"\"\"\n",
    "    retries_path = Path(retries_file)\n",
    "\n",
    "    if not retries_path.exists():\n",
    "        print(f\"Successful retries file not found: {retries_file}\")\n",
    "        return\n",
    "\n",
    "    with open(retries_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        successful_retries = json.load(file)\n",
    "\n",
    "    if not successful_retries:\n",
    "        print(\"No successful retries to apply\")\n",
    "        return\n",
    "\n",
    "    enriched_path = Path(\"./enriched_jsons\")\n",
    "    files_to_update = {}\n",
    "\n",
    "    # Group retries by filename\n",
    "    for retry in successful_retries:\n",
    "        filename = retry[\"filename\"]\n",
    "        if filename not in files_to_update:\n",
    "            files_to_update[filename] = []\n",
    "        files_to_update[filename].append(retry)\n",
    "\n",
    "    print(\n",
    "        f\"Applying {len(successful_retries)} successful retries to {len(files_to_update)} files\"\n",
    "    )\n",
    "\n",
    "    for filename, retries in files_to_update.items():\n",
    "        enriched_file_path = enriched_path / filename\n",
    "\n",
    "        if not enriched_file_path.exists():\n",
    "            print(f\"Warning: Enriched file not found: {enriched_file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Load the enriched JSON\n",
    "        with open(enriched_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            enriched_data = json.load(file)\n",
    "\n",
    "        # Apply the retry results\n",
    "        for retry in retries:\n",
    "            category_type = retry[\"category_type\"]\n",
    "            category_idx = retry[\"category_idx\"]\n",
    "            citation_idx = retry[\"citation_idx\"]\n",
    "            result = retry[\"retry_result\"]\n",
    "\n",
    "            if category_type == \"publications\":\n",
    "                enriched_data[\"publications\"][category_idx][\"citations\"][\n",
    "                    citation_idx\n",
    "                ] = result\n",
    "            else:\n",
    "                enriched_data[\"committees\"][category_idx][\"citations\"][citation_idx] = (\n",
    "                    result\n",
    "                )\n",
    "\n",
    "        # Save the updated enriched JSON\n",
    "        with open(enriched_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(enriched_data, file, indent=4, ensure_ascii=False, default=str)\n",
    "\n",
    "        print(f\"✓ Updated {filename} with {len(retries)} retry results\")\n",
    "\n",
    "    print(\"Successfully applied all retry results!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 104 JSON files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Excel files: 100%|██████████| 104/104 [00:04<00:00, 24.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "Successful: 104\n",
      "Failed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_json_to_excel(json_file_path: Path) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Process a single JSON file and create an Excel file.\n",
    "\n",
    "    Args:\n",
    "        json_file_path: Path to the JSON file to process\n",
    "\n",
    "    Returns:\n",
    "        tuple[bool, str]: (success, message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        original_name = json_data.get(\"name\", \"default_name\")\n",
    "        formatted_name = original_name.lower().replace(\" \", \"_\")\n",
    "\n",
    "        # Create excel directory if it doesn't exist\n",
    "        excel_dir = Path(\"./excel\")\n",
    "        excel_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        excel_file_name = excel_dir / f\"{formatted_name}.xlsx\"\n",
    "\n",
    "        publications_list = []\n",
    "        for pub_category in json_data.get(\"publications\", []):\n",
    "            category_name = pub_category.get(\"name\")\n",
    "            for citation in pub_category.get(\"citations\", []):\n",
    "                citation[\"tipo\"] = category_name\n",
    "                publications_list.append(citation)\n",
    "\n",
    "        df_publications = pl.DataFrame(publications_list)\n",
    "\n",
    "        pub_cols_order = [\n",
    "            \"tipo\",\n",
    "            \"ano\",\n",
    "            \"titulo\",\n",
    "            \"autores\",\n",
    "            \"periodico\",\n",
    "            \"editora\",\n",
    "            \"cidade\",\n",
    "            \"extra\",\n",
    "        ]\n",
    "\n",
    "        df_publications = df_publications.select(\n",
    "            [col for col in pub_cols_order if col in df_publications.columns]\n",
    "        )\n",
    "\n",
    "        committees_list = []\n",
    "        for com_category in json_data.get(\"committees\", []):\n",
    "            category_name = com_category.get(\"name\")\n",
    "            for citation in com_category.get(\"citations\", []):\n",
    "                citation[\"tipo\"] = category_name\n",
    "                committees_list.append(citation)\n",
    "\n",
    "        df_committees = pl.DataFrame(committees_list)\n",
    "\n",
    "        com_cols_order = [\n",
    "            \"tipo\",\n",
    "            \"ano\",\n",
    "            \"titulo\",\n",
    "            \"membros\",\n",
    "            \"candidato\",\n",
    "            \"instituicao\",\n",
    "        ]\n",
    "        df_committees = df_committees.select(\n",
    "            [col for col in com_cols_order if col in df_committees.columns]\n",
    "        )\n",
    "\n",
    "        with Workbook(str(excel_file_name)) as wb:\n",
    "            df_publications.write_excel(\n",
    "                workbook=wb,\n",
    "                worksheet=\"publicações\",\n",
    "                autofit=True,\n",
    "                column_formats={\"ano\": \"@\"},\n",
    "            )\n",
    "\n",
    "            df_committees.write_excel(\n",
    "                workbook=wb,\n",
    "                worksheet=\"bancas\",\n",
    "                autofit=True,\n",
    "                column_formats={\"ano\": \"@\"},\n",
    "            )\n",
    "\n",
    "        return True, f\"Successfully created {excel_file_name}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"Error processing {json_file_path.name}: {str(e)}\"\n",
    "\n",
    "\n",
    "def process_all_enriched_jsons():\n",
    "    \"\"\"\n",
    "    Process all JSON files in the enriched_jsons folder and create Excel files.\n",
    "    \"\"\"\n",
    "    enriched_jsons_path = Path(\"./enriched_jsons\")\n",
    "\n",
    "    if not enriched_jsons_path.exists():\n",
    "        print(\"enriched_jsons folder not found!\")\n",
    "        return\n",
    "\n",
    "    json_files = list(enriched_jsons_path.glob(\"*.json\"))\n",
    "\n",
    "    if not json_files:\n",
    "        print(\"No JSON files found in enriched_jsons folder!\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(json_files)} JSON files to process\")\n",
    "\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    failed_files = []\n",
    "\n",
    "    # Process files with progress bar\n",
    "    for json_file in tqdm(json_files, desc=\"Creating Excel files\"):\n",
    "        success, message = process_json_to_excel(json_file)\n",
    "\n",
    "        if success:\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "            failed_files.append(message)\n",
    "            print(f\"\\n  ✗ {message}\")\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "\n",
    "    if failed_files:\n",
    "        print(\"\\nFailed files:\")\n",
    "        for error_msg in failed_files:\n",
    "            print(f\"  {error_msg}\")\n",
    "\n",
    "\n",
    "# Run the processing\n",
    "process_all_enriched_jsons()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
