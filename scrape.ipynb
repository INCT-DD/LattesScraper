{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to install the required packages. They are listed in the pyproject.toml file. We recommend using uv to install them inside a virtual environment.\n",
    "\n",
    "Instead of using a browser with playwright directly, we recommend using BrightData's browser API. This way, you can avoid the captcha and the rate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import Semaphore\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from os import environ\n",
    "from playwright.async_api import Playwright, async_playwright\n",
    "\n",
    "cdp_url = environ.get(\"CDP_URL\")\n",
    "\n",
    "if not cdp_url:\n",
    "    raise Exception(\n",
    "        \"Provide Browser API credentials in CDP_URL environment variable or update the script.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a file named \"resumes.txt\" in the current folder and fill it in with the Lattes URL for each resume. Separate each link by a single new line. Ex.:\n",
    "\n",
    "```text\n",
    "http://lattes.cnpq.br/8894044025019842\n",
    "http://lattes.cnpq.br/6290785760473426\n",
    "http://lattes.cnpq.br/3201859120142095\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resumes.txt\", \"r\") as f:\n",
    "    resumes = [line.strip() for line in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semaphore is used to limit the number of concurrent requests to the browser. We recommend using 5. BrightData's browser API for certain supports more than 5 concurrent requests, but it's not guaranteed. Set a safe limit to avoid being blocked by BrightData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "semaphore = Semaphore(5)\n",
    "\n",
    "\n",
    "async def scrape(playwright: Playwright, url: str) -> tuple[bool, str, str | None]:\n",
    "    \"\"\"\n",
    "    Scrape a single Lattes resume URL using Playwright with captcha solving.\n",
    "\n",
    "    This function connects to a browser via CDP, navigates to the provided URL,\n",
    "    waits for captcha detection and solving, clicks the submit button, and\n",
    "    saves the page content as an HTML file.\n",
    "\n",
    "    Args:\n",
    "        playwright: The Playwright instance for browser automation\n",
    "        url: The Lattes resume URL to scrape\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - bool: True if scraping was successful, False otherwise\n",
    "        - str: The original URL that was scraped\n",
    "        - str | None: Error message if scraping failed, None if successful\n",
    "\n",
    "    Raises:\n",
    "        Exception: If any error occurs during the scraping process\n",
    "    \"\"\"\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            print(f\"Connecting to Browser for {url}...\")\n",
    "            browser = await playwright.chromium.connect_over_cdp(cdp_url)\n",
    "            try:\n",
    "                print(f\"Connected! Navigating to {url}...\")\n",
    "                page = await browser.new_page()\n",
    "                client = await page.context.new_cdp_session(page)\n",
    "                await page.goto(url, timeout=2 * 60_000)\n",
    "                print(f\"Navigated! Waiting captcha to detect and solve for {url}...\")\n",
    "                result = await client.send(\n",
    "                    \"Captcha.waitForSolve\",\n",
    "                    {\n",
    "                        \"detectTimeout\": 10 * 1000,\n",
    "                    },\n",
    "                )\n",
    "                status = result[\"status\"]\n",
    "                print(f\"Captcha status: {status} for {url}\")\n",
    "                if status != \"solve_finished\":\n",
    "                    return False, url, \"Captcha not solved\"\n",
    "\n",
    "                print(f\"Clicking submit button for {url}...\")\n",
    "                async with page.expect_navigation(timeout=60_000):\n",
    "                    await page.click(\"#submitBtn\")\n",
    "\n",
    "                print(f\"Extracting page content for {url}...\")\n",
    "                content = await page.content()\n",
    "\n",
    "                print(f\"Saving page content for {url}...\")\n",
    "                filename = url.split(\"/\")[-1]\n",
    "                with open(f\"./resumes/{filename}.html\", \"w+\", encoding=\"utf-8\") as f:\n",
    "                    f.write(content)\n",
    "\n",
    "                print(f\"Page saved successfully for {url}!\")\n",
    "                return True, url, None\n",
    "\n",
    "            finally:\n",
    "                await browser.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error scraping {url}: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return False, url, str(e)\n",
    "\n",
    "\n",
    "async def scrape_all_urls(urls: list[str]) -> tuple[list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Scrape multiple Lattes resume URLs concurrently with progress tracking.\n",
    "\n",
    "    This function creates concurrent scraping tasks for all provided URLs,\n",
    "    executes them with a progress bar, and categorizes results into\n",
    "    successful and failed URLs.\n",
    "\n",
    "    Args:\n",
    "        urls: List of Lattes resume URLs to scrape\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - list[str]: URLs that were successfully scraped\n",
    "        - list[str]: URLs that failed to be scraped\n",
    "\n",
    "    Note:\n",
    "        Uses a semaphore to limit concurrent requests and avoid overwhelming\n",
    "        the browser API service.\n",
    "    \"\"\"\n",
    "    async with async_playwright() as playwright:\n",
    "        tasks = [scrape(playwright, url) for url in urls]\n",
    "\n",
    "        results = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "        successful_urls = []\n",
    "        failed_urls = []\n",
    "\n",
    "        for result in results:\n",
    "            if isinstance(result, Exception):\n",
    "                # This shouldn't happen since we handle exceptions in scrape()\n",
    "                print(f\"Unexpected error: {result}\")\n",
    "                failed_urls.append(\"unknown_url\")\n",
    "            else:\n",
    "                success, url, error = result\n",
    "                if success:\n",
    "                    successful_urls.append(url)\n",
    "                else:\n",
    "                    failed_urls.append(url)\n",
    "\n",
    "        return successful_urls, failed_urls\n",
    "\n",
    "\n",
    "def save_failed_urls(failed_urls: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Save a list of failed URLs to the failed_extractions.txt file.\n",
    "\n",
    "    This function writes each failed URL on a separate line in the\n",
    "    failed_extractions.txt file for later retry attempts.\n",
    "\n",
    "    Args:\n",
    "        failed_urls: List of URLs that failed during scraping\n",
    "\n",
    "    Note:\n",
    "        If the failed_urls list is empty, no file is created and a\n",
    "        message is printed indicating no failures occurred.\n",
    "    \"\"\"\n",
    "    if failed_urls:\n",
    "        with open(\"failed_extractions.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
    "            for url in failed_urls:\n",
    "                f.write(f\"{url}\\n\")\n",
    "        print(f\"Saved {len(failed_urls)} failed URLs to failed_extractions.txt\")\n",
    "    else:\n",
    "        print(\"No failed URLs to save!\")\n",
    "\n",
    "\n",
    "def load_failed_urls() -> list[str]:\n",
    "    \"\"\"\n",
    "    Load previously failed URLs from the failed_extractions.txt file.\n",
    "\n",
    "    This function reads the failed_extractions.txt file and returns\n",
    "    a list of URLs that can be retried in subsequent scraping attempts.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of URLs that previously failed to scrape.\n",
    "                  Returns empty list if file doesn't exist or is empty.\n",
    "\n",
    "    Note:\n",
    "        Strips whitespace from each line and filters out empty lines.\n",
    "        If the file doesn't exist, returns an empty list and prints\n",
    "        a notification message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(\"failed_extractions.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            return [line.strip() for line in f.readlines() if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(\"No failed_extractions.txt file found\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with async_playwright() as playwright:\n",
    "    print(await scrape(playwright, \"http://lattes.cnpq.br/8894044025019842\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_urls, failed_urls = await scrape_all_urls(resumes)\n",
    "\n",
    "print(f\"Successfully scraped: {len(successful_urls)} URLs\")\n",
    "print(f\"Failed to scrape: {len(failed_urls)} URLs\")\n",
    "\n",
    "save_failed_urls(failed_urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we did not implement a way to automatically retry failed URLs. You can do it manually by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_urls = load_failed_urls()\n",
    "\n",
    "if failed_urls:\n",
    "    print(f\"Retrying {len(failed_urls)} failed URLs...\")\n",
    "    successful_retry, still_failed = await scrape_all_urls(failed_urls)\n",
    "\n",
    "    print(\n",
    "        f\"Retry results - Success: {len(successful_retry)}, Still failed: {len(still_failed)}\"\n",
    "    )\n",
    "\n",
    "    save_failed_urls(still_failed)\n",
    "else:\n",
    "    print(\"No failed URLs to retry\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
